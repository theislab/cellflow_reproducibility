{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "021bbbf6-834e-4580-b01a-6d69e4b2bfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icb/dominik.klein/mambaforge/envs/cfp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import optax\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from ott.neural.networks.layers.posdef import PositiveDense\n",
    "from types import MappingProxyType\n",
    "from typing import TYPE_CHECKING, Any, Callable, Dict, Optional, Sequence, Tuple, Iterable, Type, Mapping\n",
    "\n",
    "from collections import defaultdict, abc\n",
    "from types import MappingProxyType\n",
    "from typing import Any, Callable, Dict, Iterable, List, Literal, Optional, Tuple, Union, Type\n",
    "\n",
    "\n",
    "import optax\n",
    "from flax.core import freeze\n",
    "from flax.core.scope import FrozenVariableDict\n",
    "from flax.training.train_state import TrainState\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from ott.geometry import costs\n",
    "from ott.geometry.pointcloud import PointCloud\n",
    "\n",
    "import cfp\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import functools\n",
    "from ott.solvers import utils as solver_utils\n",
    "import optax\n",
    "from omegaconf import OmegaConf\n",
    "from typing import NamedTuple, Any\n",
    "\n",
    "from cfp.data._data import ConditionData, ValidationData\n",
    "from cfp.data._dataloader import PredictionSampler, TrainSampler, ValidationSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "802ccaa8-fb2d-4da0-a241-8eced373a72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class ICNN(nn.Module):\n",
    "    \"\"\"Input convex neural network (ICNN) architecture.\"\"\"\n",
    "\n",
    "    dim_hidden: Sequence[int]\n",
    "    input_dim: int\n",
    "    cond_dim: int\n",
    "    init_std: float = 0.1\n",
    "    init_fn: Callable[[jnp.ndarray], Callable[[jnp.ndarray], jnp.ndarray]] = nn.initializers.normal  # type: ignore[name-defined]  # noqa: E501\n",
    "    act_fn: Callable[[jnp.ndarray], jnp.ndarray] = nn.leaky_relu  # type: ignore[name-defined]\n",
    "    pos_weights: bool = False\n",
    "\n",
    "    def setup(self):\n",
    "        \"\"\"Initialize ICNN architecture.\"\"\"\n",
    "        num_hidden = len(self.dim_hidden)\n",
    "\n",
    "        Dense = PositiveDense if self.pos_weights else nn.Dense\n",
    "        kernel_inits_wz = [self.init_fn(self.init_std) for _ in range(num_hidden + 1)]\n",
    "\n",
    "        w_xs = []\n",
    "        w_zs = []\n",
    "        for i in range(0, num_hidden):\n",
    "            w_xs.append(\n",
    "                nn.Dense(\n",
    "                    self.dim_hidden[i],\n",
    "                    kernel_init=self.init_fn(self.init_std),\n",
    "                    bias_init=self.init_fn(self.init_std),\n",
    "                    use_bias=True,\n",
    "                )\n",
    "            )\n",
    "            if i != 0:\n",
    "                w_zs.append(\n",
    "                    Dense(\n",
    "                        self.dim_hidden[i],\n",
    "                        kernel_init=kernel_inits_wz[i],\n",
    "                        use_bias=False,\n",
    "                    )\n",
    "                )\n",
    "        w_xs.append(\n",
    "            nn.Dense(\n",
    "                1,\n",
    "                kernel_init=self.init_fn(self.init_std),\n",
    "                bias_init=self.init_fn(self.init_std),\n",
    "                use_bias=True,\n",
    "            )\n",
    "        )\n",
    "        w_zs.append(Dense(1, kernel_init=kernel_inits_wz[-1], use_bias=False))\n",
    "        self.w_xs = w_xs\n",
    "        self.w_zs = w_zs\n",
    "\n",
    "        if self.cond_dim:\n",
    "            w_zu = []\n",
    "            w_xu = []\n",
    "            w_u = []\n",
    "            v = []\n",
    "\n",
    "            for i in range(0, num_hidden):\n",
    "                if i != 0:\n",
    "                    w_zu.append(\n",
    "                        nn.Dense(\n",
    "                            self.dim_hidden[i],\n",
    "                            kernel_init=self.init_fn(self.init_std),\n",
    "                            use_bias=True,\n",
    "                            bias_init=self.init_fn(self.init_std),\n",
    "                        )\n",
    "                    )\n",
    "                w_xu.append(  # this the matrix that multiply with x\n",
    "                    nn.Dense(\n",
    "                        self.input_dim,  # self.dim_hidden[i],\n",
    "                        kernel_init=self.init_fn(self.init_std),\n",
    "                        use_bias=True,\n",
    "                        bias_init=self.init_fn(self.init_std),\n",
    "                    )\n",
    "                )\n",
    "                w_u.append(\n",
    "                    nn.Dense(\n",
    "                        self.dim_hidden[i],\n",
    "                        kernel_init=self.init_fn(self.init_std),\n",
    "                        use_bias=True,\n",
    "                        bias_init=self.init_fn(self.init_std),\n",
    "                    )\n",
    "                )\n",
    "                v.append(\n",
    "                    nn.Dense(\n",
    "                        2,\n",
    "                        kernel_init=self.init_fn(self.init_std),\n",
    "                        use_bias=True,\n",
    "                        bias_init=self.init_fn(self.init_std),\n",
    "                    )\n",
    "                )\n",
    "            w_zu.append(\n",
    "                nn.Dense(\n",
    "                    self.dim_hidden[-1],\n",
    "                    kernel_init=self.init_fn(self.init_std),\n",
    "                    use_bias=True,\n",
    "                    bias_init=self.init_fn(self.init_std),\n",
    "                )\n",
    "            )\n",
    "            w_xu.append(  # this the matrix that multiply with x\n",
    "                nn.Dense(\n",
    "                    self.input_dim,\n",
    "                    kernel_init=self.init_fn(self.init_std),\n",
    "                    use_bias=True,\n",
    "                    bias_init=self.init_fn(self.init_std),\n",
    "                )\n",
    "            )\n",
    "            w_u.append(\n",
    "                nn.Dense(\n",
    "                    1,\n",
    "                    kernel_init=self.init_fn(self.init_std),\n",
    "                    bias_init=self.init_fn(self.init_std),\n",
    "                    use_bias=True,\n",
    "                )\n",
    "            )\n",
    "            v.append(\n",
    "                nn.Dense(\n",
    "                    1,\n",
    "                    kernel_init=self.init_fn(self.init_std),\n",
    "                    bias_init=self.init_fn(self.init_std),\n",
    "                    use_bias=True,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            self.w_zu = w_zu\n",
    "            self.w_xu = w_xu\n",
    "            self.w_u = w_u\n",
    "            self.v = v\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: jnp.ndarray, c: Optional[jnp.ndarray] = None) -> jnp.ndarray:  # type: ignore[name-defined]\n",
    "        \"\"\"Apply ICNN module.\"\"\"\n",
    "        assert (c is not None) == (self.cond_dim > 0), \"`conditional` flag and whether `c` is provided must match.\"\n",
    "\n",
    "        if not self.cond_dim:\n",
    "            z = self.w_xs[0](x)\n",
    "            z = jnp.multiply(z, z)\n",
    "            for Wz, Wx in zip(self.w_zs[:-1], self.w_xs[1:-1]):\n",
    "                z = self.act_fn(jnp.add(Wz(z), Wx(x)))\n",
    "            y = jnp.add(self.w_zs[-1](z), self.w_xs[-1](x))\n",
    "        else:\n",
    "            # Initialize\n",
    "            mlp_condition_embedding = self.w_xu[0](c)\n",
    "            x_hadamard_1 = jnp.multiply(x, mlp_condition_embedding)\n",
    "            mlp_condition = self.w_u[0](c)\n",
    "            z = jnp.add(mlp_condition, self.w_xs[0](x_hadamard_1))\n",
    "            z = jnp.multiply(z, z)\n",
    "            u = self.act_fn(self.v[0](c))\n",
    "\n",
    "            for Wz, Wx, Wzu, Wxu, Wu, V in zip(\n",
    "                self.w_zs[:-1], self.w_xs[:-1], self.w_zu[:-1], self.w_xu[1:-1], self.w_u[1:-1], self.v[1:-1]\n",
    "            ):\n",
    "                mlp_convex = jnp.clip(Wzu(u), a_min=0)\n",
    "                z_hadamard_1 = jnp.multiply(z, mlp_convex)\n",
    "                mlp_condition_embedding = Wxu(u)\n",
    "                x_hadamard_1 = jnp.multiply(x, mlp_condition_embedding)\n",
    "                mlp_condition = Wu(u)\n",
    "                z = self.act_fn(jnp.add(jnp.add(Wz(z_hadamard_1), Wx(x_hadamard_1)), mlp_condition))\n",
    "                u = self.act_fn(V(u))\n",
    "\n",
    "            mlp_convex = jnp.clip(self.w_zu[-1](u), a_min=0)  # bs x d\n",
    "            z_hadamard_1 = jnp.multiply(z, mlp_convex)  # bs x d\n",
    "\n",
    "            mlp_condition_embedding = self.w_xu[-1](u)  # bs x d\n",
    "            x_hadamard_1 = jnp.multiply(x, mlp_condition_embedding)  # bs x d\n",
    "\n",
    "            mlp_condition = self.w_u[-1](u)\n",
    "            y = jnp.add(jnp.add(self.w_zs[-1](z_hadamard_1), self.w_xs[-1](x_hadamard_1)), mlp_condition)\n",
    "\n",
    "        return jnp.squeeze(y, axis=-1)\n",
    "\n",
    "    def create_train_state(\n",
    "        self,\n",
    "        rng: jnp.ndarray,  # type: ignore[name-defined]\n",
    "        optimizer: optax.OptState,\n",
    "        input_shape: Union[int, Tuple[int, ...]],\n",
    "    ) -> train_state.TrainState:\n",
    "        \"\"\"Create initial `TrainState`.\"\"\"\n",
    "        condition = (\n",
    "            jnp.ones(\n",
    "                shape=[\n",
    "                    self.cond_dim,\n",
    "                ]\n",
    "            )\n",
    "            if self.cond_dim\n",
    "            else None\n",
    "        )\n",
    "        params = self.init(rng, x=jnp.ones(input_shape), c=condition)[\"params\"]\n",
    "        return train_state.TrainState.create(apply_fn=self.apply, params=params, tx=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dcbf42f-2905-474d-bed0-577f899f7973",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "Train_t = Dict[str, Dict[str, Union[float, List[float]]]]\n",
    "\n",
    "\n",
    "def _get_icnn(\n",
    "    input_dim: int,\n",
    "    cond_dim: int,\n",
    "    pos_weights: bool = False,\n",
    "    dim_hidden: Iterable[int] = (64, 64, 64, 64),\n",
    "    **kwargs: Any,\n",
    ") -> ICNN:\n",
    "    return ICNN(input_dim=input_dim, cond_dim=cond_dim, pos_weights=pos_weights, dim_hidden=dim_hidden, **kwargs)\n",
    "\n",
    "\n",
    "def _get_optimizer(\n",
    "    learning_rate: float = 1e-4, b1: float = 0.5, b2: float = 0.9, weight_decay: float = 0.0, **kwargs: Any\n",
    ") -> Type[optax.GradientTransformation]:\n",
    "    return optax.adamw(learning_rate=learning_rate, b1=b1, b2=b2, weight_decay=weight_decay, **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cda84a39-f56d-41a3-aaa3-cb28e0c6aba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class OTTNeuralDualSolver:\n",
    "    \"\"\"Solver of the ICNN-based Kantorovich dual.\n",
    "\n",
    "    Optimal transport mapping via input convex neural networks,\n",
    "    Makkuva-Taghvaei-Lee-Oh, ICML'20.\n",
    "    http://proceedings.mlr.press/v119/makkuva20a/makkuva20a.pdf\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_dim\n",
    "        Input dimension of data (without condition)\n",
    "    conditional\n",
    "        Whether to use partial input convex neural networks (:cite:`bunne2022supervised`).\n",
    "    batch_size\n",
    "        Batch size.\n",
    "    tau_a\n",
    "        Unbalancedness parameter in the source distribution in the inner sampling loop.\n",
    "    tau_b\n",
    "        Unbalancedness parameter in the target distribution in the inner sampling loop.\n",
    "    epsilon\n",
    "        Entropic regularisation parameter in the inner sampling loop.\n",
    "    seed\n",
    "        Seed for splitting the data.\n",
    "    pos_weights\n",
    "        If `True` enforces non-negativity of corresponding weights of ICNNs, else only penalizes negativity.\n",
    "    dim_hidden\n",
    "        The length of `dim_hidden` determines the depth of the ICNNs, while the entries of the list determine\n",
    "        the layer widhts.\n",
    "    beta\n",
    "        If `pos_weights` is not `None`, this determines the multiplicative constant of L2-penalization of\n",
    "        negative weights in ICNNs.\n",
    "    best_model_metric\n",
    "        Which metric to use to assess model training. The specified metric needs to be computed in the passed\n",
    "        `callback_func`. By default `sinkhorn_loss_forward` only takes into account the error in the forward map,\n",
    "        while `sinkhorn` computes the mean error between the forward and the inverse map.\n",
    "    iterations\n",
    "        Number of (outer) training steps (batches) of the training process.\n",
    "    inner_iters\n",
    "        Number of inner iterations for updating the convex conjugate.\n",
    "    valid_freq\n",
    "        Frequency at which the model is evaluated.\n",
    "    log_freq\n",
    "        Frequency at which training is logged.\n",
    "    patience\n",
    "        Number of iterations of no performance increase after which to apply early stopping.\n",
    "    optimizer_f_kwargs\n",
    "        Keyword arguments for the optimizer :class:`optax.adamw` for f.\n",
    "    optimizer_g_kwargs\n",
    "        Keyword arguments for the optimizer :class:`optax.adamw` for g.\n",
    "    pretrain_iters\n",
    "        Number of iterations (batches) for pretraining with the identity map.\n",
    "    pretrain_scale\n",
    "        Variance of Gaussian distribution used for pretraining.\n",
    "    sinkhorn_kwargs\n",
    "        Keyword arguments for computing the discrete sinkhorn divergence for assessing model training.\n",
    "        By default, the same `tau_a`, `tau_b` and `epsilon` are taken as for the inner sampling loop.\n",
    "    compute_wasserstein_baseline\n",
    "        Whether to compute the Sinkhorn divergence between the source and the target distribution as\n",
    "        a baseline for the Wasserstein-2 distance computed with the neural solver.\n",
    "    callback_func\n",
    "        Callback function to compute metrics during training. The function takes as input the\n",
    "        target and source batch and the predicted target and source batch and returns a dictionary of\n",
    "        metrics.\n",
    "\n",
    "    Warning\n",
    "    -------\n",
    "    If `compute_wasserstein_distance` is `True`, a discrete OT problem has to be solved on the validation\n",
    "    dataset which scales linearly in the validation set size. If `train_size=1.0` the validation dataset size\n",
    "    is the full dataset size, hence this is a source of prolonged run time or Out of Memory Error.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        cond_dim: int = 0,\n",
    "        batch_size: int = 1024,\n",
    "        tau_a: float = 1.0,\n",
    "        tau_b: float = 1.0,\n",
    "        epsilon: float = 0.1,\n",
    "        seed: int = 0,\n",
    "        pos_weights: bool = False,\n",
    "        f: Union[Dict[str, Any], ICNN] = MappingProxyType({}),\n",
    "        g: Union[Dict[str, Any], ICNN] = MappingProxyType({}),\n",
    "        beta: float = 1.0,\n",
    "        iterations: int = 25000,  # TODO(@MUCDK): rename to max_iterations\n",
    "        inner_iters: int = 10,\n",
    "        valid_freq: int = 250,\n",
    "        log_freq: int = 10,\n",
    "        patience: int = 100,\n",
    "        optimizer_f: Union[Dict[str, Any], Type[optax.GradientTransformation]] = MappingProxyType({}),\n",
    "        optimizer_g: Union[Dict[str, Any], Type[optax.GradientTransformation]] = MappingProxyType({}),\n",
    "        pretrain_iters: int = 15001,\n",
    "        pretrain_scale: float = 3.0,\n",
    "        valid_sinkhorn_kwargs: Dict[str, Any] = MappingProxyType({}),\n",
    "        compute_wasserstein_baseline: bool = True,\n",
    "        callback_func: Optional[\n",
    "            Callable[[jnp.ndarray, jnp.ndarray, jnp.ndarray, jnp.ndarray], Dict[str, float]]\n",
    "        ] = None,\n",
    "    ):\n",
    "        self.input_dim = input_dim\n",
    "        self.cond_dim = cond_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.tau_a = 1.0 if tau_a is None else tau_a\n",
    "        self.tau_b = 1.0 if tau_b is None else tau_b\n",
    "        self.epsilon = epsilon if self.tau_a != 1.0 or self.tau_b != 1.0 else None\n",
    "        self.pos_weights = pos_weights\n",
    "        self.beta = beta\n",
    "        self.iterations = iterations\n",
    "        self.inner_iters = inner_iters\n",
    "        self.valid_freq = valid_freq\n",
    "        self.log_freq = log_freq\n",
    "        self.patience = patience\n",
    "        self.pretrain_iters = pretrain_iters\n",
    "        self.pretrain_scale = pretrain_scale\n",
    "        self.key: jax.random.PRNGKeyArray = jax.random.PRNGKey(seed)\n",
    "\n",
    "        self.optimizer_f = _get_optimizer(**optimizer_f) if isinstance(optimizer_f, abc.Mapping) else optimizer_f\n",
    "        self.optimizer_g = _get_optimizer(**optimizer_g) if isinstance(optimizer_g, abc.Mapping) else optimizer_g\n",
    "        self.neural_f = _get_icnn(input_dim=input_dim, cond_dim=cond_dim, **f) if isinstance(f, abc.Mapping) else f\n",
    "        self.neural_g = _get_icnn(input_dim=input_dim, cond_dim=cond_dim, **g) if isinstance(g, abc.Mapping) else g\n",
    "        self.callback_func = callback_func\n",
    "        \n",
    "        # set optimizer and networks\n",
    "        self.setup(self.neural_f, self.neural_g, self.optimizer_f, self.optimizer_g)\n",
    "\n",
    "    def setup(self, neural_f: ICNN, neural_g: ICNN, optimizer_f: optax.OptState, optimizer_g: optax.OptState):\n",
    "        \"\"\"Initialize all components required to train the :class:`moscot.backends.ott.NeuralDual`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        neural_f\n",
    "            Network to parameterize the forward transport map.\n",
    "        neural_g\n",
    "            Network to parameterize the reverse transport map.\n",
    "        optimizer_f\n",
    "            Optimizer for `neural_f`.\n",
    "        optimizer_g\n",
    "            Optimizer for `neural_g`.\n",
    "        \"\"\"\n",
    "        key_f, key_g, self.key = jax.random.split(self.key, 3)  # type:ignore[arg-type]\n",
    "\n",
    "        # check setting of network architectures\n",
    "        if neural_g.pos_weights != self.pos_weights or neural_f.pos_weights != self.pos_weights:\n",
    "            logger.warning(\n",
    "                f\"Setting of ICNN and the positive weights setting of the \\\n",
    "                      `NeuralDualSolver` are not consistent. Proceeding with \\\n",
    "                      the `NeuralDualSolver` setting, with positive weigths \\\n",
    "                      being {self.pos_weights}.\"\n",
    "            )\n",
    "            neural_g.pos_weights = self.pos_weights\n",
    "            neural_f.pos_weights = self.pos_weights\n",
    "\n",
    "        self.state_f = neural_f.create_train_state(key_f, optimizer_f, self.input_dim)\n",
    "        self.state_g = neural_g.create_train_state(key_g, optimizer_g, self.input_dim)\n",
    "\n",
    "        self.train_step_f = self.get_train_step(to_optimize=\"f\")\n",
    "        self.train_step_g = self.get_train_step(to_optimize=\"g\")\n",
    "\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        trainloader,\n",
    "        N_PCs: int,\n",
    "    ) -> Any:\n",
    "        \"\"\"Start the training pipeline of the :class:`moscot.backends.ott.NeuralDual`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        trainloader\n",
    "            Data loader for the training data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        The trained model and training statistics.\n",
    "        \"\"\"\n",
    "        pretrain_logs = {}\n",
    "        if self.pretrain_iters > 0:\n",
    "            condition_arr = jnp.squeeze(jnp.concatenate((train_dataloader._data.condition_data[\"drugs\"], train_dataloader._data.condition_data[\"dose\"], train_dataloader._data.condition_data[\"cell_line\"]), axis=-1))\n",
    "            pretrain_logs = self.pretrain_identity(condition_arr)\n",
    "\n",
    "        self.train_neuraldual(trainloader, N_PCs)\n",
    "    \n",
    "    \n",
    "        return None\n",
    "\n",
    "    def pretrain_identity(\n",
    "        self, conditions: Optional[jnp.ndarray]  # type:ignore[name-defined]\n",
    "    ) -> Train_t:  # TODO(@lucaeyr) conditions can be `None` right?\n",
    "        \"\"\"Pretrain the neural networks to parameterize the identity map.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        conditions\n",
    "            Conditions in the case of a conditional Neural OT model, otherwise `None`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Pre-training statistics.\n",
    "        \"\"\"\n",
    "\n",
    "        def pretrain_loss_fn(\n",
    "            params: jnp.ndarray,  # type: ignore[name-defined]\n",
    "            data: jnp.ndarray,  # type: ignore[name-defined]\n",
    "            condition: jnp.ndarray,  # type: ignore[name-defined]\n",
    "            state: TrainState,\n",
    "        ) -> float:\n",
    "            \"\"\"Loss function for the pretraining on identity.\"\"\"\n",
    "            grad_g_data = jax.vmap(jax.grad(lambda x: state.apply_fn({\"params\": params}, x, condition), argnums=0))(\n",
    "                data\n",
    "            )\n",
    "            # loss is L2 reconstruction of the input\n",
    "            return ((grad_g_data - data) ** 2).sum(axis=1).mean()  # TODO make nicer\n",
    "\n",
    "        @jax.jit\n",
    "        def pretrain_update(\n",
    "            state: TrainState, key: jax.Array\n",
    "        ) -> Tuple[jnp.ndarray, TrainState]:  # type:ignore[name-defined]\n",
    "            \"\"\"Update function for the pretraining on identity.\"\"\"\n",
    "            # sample gaussian data with given scale\n",
    "            x = self.pretrain_scale * jax.random.normal(key, [self.batch_size, self.input_dim])\n",
    "            condition = jax.random.choice(key, conditions) if self.cond_dim else None  # type:ignore[arg-type]\n",
    "            grad_fn = jax.value_and_grad(pretrain_loss_fn, argnums=0)\n",
    "            loss, grads = grad_fn(state.params, x, condition, state)\n",
    "            return loss, state.apply_gradients(grads=grads)\n",
    "\n",
    "        pretrain_logs: Dict[str, List[float]] = {\"loss\": []}\n",
    "        for iteration in tqdm(range(self.pretrain_iters)):\n",
    "            key_pre, self.key = jax.random.split(self.key, 2)  # type:ignore[arg-type]\n",
    "            # train step for potential g directly updating the train state\n",
    "            loss, self.state_g = pretrain_update(self.state_g, key_pre)\n",
    "            # clip weights of g\n",
    "            if not self.pos_weights:\n",
    "                self.state_g = self.state_g.replace(params=self.clip_weights_icnn(self.state_g.params))\n",
    "            if iteration % self.log_freq == 0:\n",
    "                pretrain_logs[\"loss\"].append(loss)\n",
    "        # load params of g into state_f\n",
    "        # this only works when f & g have the same architecture\n",
    "        self.state_f = self.state_f.replace(params=self.state_g.params)\n",
    "        return {\"pretrain_logs\": pretrain_logs}  # type:ignore[dict-item]\n",
    "\n",
    "    def train_neuraldual(\n",
    "        self,\n",
    "        dataloader,\n",
    "        N_PCs,\n",
    "    ) -> Train_t:\n",
    "        \"\"\"Train the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        trainloader\n",
    "            Data loader for the training data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Training statistics.\n",
    "        \"\"\"\n",
    "        # set logging dictionaries\n",
    "        train_logs: Dict[str, List[float]] = defaultdict(list)\n",
    "        valid_logs: Dict[str, Union[List[float], float]] = defaultdict(list)\n",
    "        sink_dist: List[float] = []\n",
    "        curr_patience: int = 0\n",
    "        best_loss: float = jnp.inf\n",
    "        best_iter_distance: float = jnp.inf\n",
    "        best_params_f: jnp.ndarray = self.state_f.params\n",
    "        best_params_g: jnp.ndarray = self.state_g.params  # type:ignore[name-defined]\n",
    "\n",
    "        # define dict to contain source and target batch\n",
    "        batch: Dict[str, jnp.ndarray] = {}  # type:ignore[name-defined]\n",
    "        \n",
    "        for iteration in tqdm(range(self.iterations)):\n",
    "            # sample policy and condition if given in trainloader\n",
    "            key, self.key = jax.random.split(self.key, 2)  # type:ignore[arg-type]\n",
    "            batch = dataloader.sample(key)\n",
    "            \n",
    "            batch[\"source\"]=batch[\"src_cell_data\"][:,:N_PCs]\n",
    "            batch[\"target\"]= batch[\"tgt_cell_data\"][:,:N_PCs]\n",
    "            condition_arr = jnp.concatenate((train_dataloader._data.condition_data[\"drugs\"], train_dataloader._data.condition_data[\"dose\"], train_dataloader._data.condition_data[\"cell_line\"]), axis=-1)\n",
    "            batch[\"condition\"] = jnp.squeeze(condition_arr[0,...])\n",
    "\n",
    "           \n",
    "            self.state_f, train_f_metrics = self.train_step_f(self.state_f, self.state_g, batch)\n",
    "            self.state_g, train_g_metrics = self.train_step_g(self.state_f, self.state_g, batch)\n",
    "            if not self.pos_weights:\n",
    "                self.state_g = self.state_g.replace(params=self.clip_weights_icnn(self.state_g.params))\n",
    "            \n",
    "    def get_train_step(\n",
    "        self,\n",
    "        to_optimize: Literal[\"f\", \"g\"],\n",
    "    ) -> Callable[  # type:ignore[name-defined]\n",
    "        [TrainState, TrainState, Dict[str, jnp.ndarray]], Tuple[TrainState, Dict[str, float]]\n",
    "    ]:\n",
    "        \"\"\"Get one training step.\"\"\"\n",
    "\n",
    "        def loss_f_fn(\n",
    "            params_f: jnp.ndarray,  # type:ignore[name-defined]\n",
    "            params_g: jnp.ndarray,  # type:ignore[name-defined]\n",
    "            state_f: TrainState,\n",
    "            state_g: TrainState,\n",
    "            batch: Dict[str, jnp.ndarray],  # type:ignore[name-defined]\n",
    "        ) -> Tuple[jnp.ndarray, List[jnp.ndarray]]:  # type:ignore[name-defined]\n",
    "            \"\"\"Loss function for f.\"\"\"\n",
    "            # get loss terms of kantorovich dual\n",
    "            grad_f_src = jax.vmap(\n",
    "                jax.grad(lambda x: state_f.apply_fn({\"params\": params_f}, x, batch[\"condition\"]), argnums=0)\n",
    "            )(batch[\"source\"])\n",
    "            g_grad_f_src = jax.vmap(lambda x: state_g.apply_fn({\"params\": params_g}, x, batch[\"condition\"]))(grad_f_src)\n",
    "            src_dot_grad_f_src = jnp.sum(batch[\"source\"] * grad_f_src, axis=1)\n",
    "            # compute loss\n",
    "            loss = jnp.mean(g_grad_f_src - src_dot_grad_f_src)\n",
    "            if not self.pos_weights:\n",
    "                penalty = self.beta * self.penalize_weights_icnn(params_f)\n",
    "                loss += penalty\n",
    "            else:\n",
    "                penalty = 0\n",
    "            return loss, [penalty]\n",
    "\n",
    "        def loss_g_fn(\n",
    "            params_f: jnp.ndarray,  # type:ignore[name-defined]\n",
    "            params_g: jnp.ndarray,  # type:ignore[name-defined]\n",
    "            state_f: TrainState,\n",
    "            state_g: TrainState,\n",
    "            batch: Dict[str, jnp.ndarray],  # type:ignore[name-defined]\n",
    "        ) -> Tuple[jnp.ndarray, List[float]]:  # type: ignore[name-defined]\n",
    "            \"\"\"Loss function for g.\"\"\"\n",
    "            # get loss terms of kantorovich dual\n",
    "            grad_f_src = jax.vmap(\n",
    "                jax.grad(lambda x: state_f.apply_fn({\"params\": params_f}, x, batch[\"condition\"]), argnums=0)\n",
    "            )(batch[\"source\"])\n",
    "            g_grad_f_src = jax.vmap(lambda x: state_g.apply_fn({\"params\": params_g}, x, batch[\"condition\"]))(grad_f_src)\n",
    "            src_dot_grad_f_src = jnp.sum(batch[\"source\"] * grad_f_src, axis=1)\n",
    "            # compute loss\n",
    "            g_tgt = jax.vmap(lambda x: state_g.apply_fn({\"params\": params_g}, x, batch[\"condition\"]))(batch[\"target\"])\n",
    "            loss = jnp.mean(g_tgt - g_grad_f_src)\n",
    "            total_loss = jnp.mean(g_grad_f_src - g_tgt - src_dot_grad_f_src)\n",
    "            # compute wasserstein distance\n",
    "            dist = 2 * total_loss + jnp.mean(\n",
    "                jnp.sum(batch[\"target\"] * batch[\"target\"], axis=1)\n",
    "                + 0.5 * jnp.sum(batch[\"source\"] * batch[\"source\"], axis=1)\n",
    "            )\n",
    "            return loss, [total_loss, dist]\n",
    "\n",
    "        @jax.jit\n",
    "        def step_fn(\n",
    "            state_f: TrainState,\n",
    "            state_g: TrainState,\n",
    "            batch: Dict[str, jnp.ndarray],  # type: ignore[name-defined]\n",
    "        ) -> Tuple[TrainState, Dict[str, float]]:\n",
    "            \"\"\"Step function for training.\"\"\"\n",
    "            # get loss function for f or g\n",
    "            if to_optimize == \"f\":\n",
    "                grad_fn = jax.value_and_grad(loss_f_fn, argnums=0, has_aux=True)\n",
    "                # compute loss, gradients and metrics\n",
    "                (loss, raw_metrics), grads = grad_fn(state_f.params, state_g.params, state_f, state_g, batch)\n",
    "                # return updated state and metrics dict\n",
    "                metrics = {\"loss_f\": loss, \"penalty\": raw_metrics[0]}\n",
    "                return state_f.apply_gradients(grads=grads), metrics\n",
    "            if to_optimize == \"g\":\n",
    "                grad_fn = jax.value_and_grad(loss_g_fn, argnums=1, has_aux=True)\n",
    "                # compute loss, gradients and metrics\n",
    "                (loss, raw_metrics), grads = grad_fn(state_f.params, state_g.params, state_f, state_g, batch)\n",
    "                # return updated state and metrics dict\n",
    "                metrics = {\"loss_g\": loss, \"loss\": raw_metrics[0], \"w_dist\": raw_metrics[1]}\n",
    "                return state_g.apply_gradients(grads=grads), metrics\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        return step_fn\n",
    "\n",
    "\n",
    "    def clip_weights_icnn(self, params: FrozenVariableDict) -> FrozenVariableDict:\n",
    "        \"\"\"Clip weights of ICNN.\"\"\"\n",
    "        for key in params:\n",
    "            if key.startswith(\"w_zs\"):\n",
    "                params[key][\"kernel\"] = jnp.clip(params[key][\"kernel\"], a_min=0)\n",
    "\n",
    "        return params\n",
    "\n",
    "    def penalize_weights_icnn(self, params: FrozenVariableDict) -> float:\n",
    "        \"\"\"Penalize weights of ICNN.\"\"\"\n",
    "        penalty = 0\n",
    "        for key in params:\n",
    "            if key.startswith(\"w_z\"):\n",
    "                penalty += jnp.linalg.norm(jax.nn.relu(-params[key][\"kernel\"]))\n",
    "        return penalty\n",
    "\n",
    "\n",
    "    @property\n",
    "    def is_balanced(self) -> bool:\n",
    "        \"\"\"Return whether the problem is balanced.\"\"\"\n",
    "        return self.tau_a == self.tau_b == 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fa23006-4178-430a-af8a-7b5fbf1403ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/home/icb/dominik.klein/git_repos/ot_pert_new/competing_methods/sciplex/CondOT/results_hsearch.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ac9c84a-8229-4e23-9e97-79ac7b16c257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                 10.000000\n",
       "Unnamed: 0            10.000000\n",
       "N_PCs                 50.000000\n",
       "pretrain_iters     10000.000000\n",
       "iterations        200000.000000\n",
       "batch_size           256.000000\n",
       "r2_ood                 0.769893\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_vals = df.sort_values(\"r2_ood\", ascending=False).reset_index().loc[0]\n",
    "best_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "894fa89a-73c8-4c84-b9e0-c79d6249090c",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PCs = int(best_vals[\"N_PCs\"])\n",
    "pretrain_iters = int(best_vals[\"pretrain_iters\"])\n",
    "iterations = int(best_vals[\"iterations\"])\n",
    "batch_size = int(best_vals[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b02ec7db-5d87-4929-8203-7c6089cc192a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ictstr01/home/icb/dominik.klein/git_repos/cell_flow_perturbation/src/cfp/data/_datamanager.py:334: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  _covariate_data[\"cell_index\"] = _covariate_data.index\n",
      "100%|██████████| 677/677 [00:16<00:00, 41.32it/s]\n",
      "100%|██████████| 708/708 [00:15<00:00, 44.70it/s]\n",
      "100%|██████████| 660/660 [00:13<00:00, 47.98it/s]\n"
     ]
    }
   ],
   "source": [
    "split=5\n",
    "adata_train_path = f\"/lustre/groups/ml01/workspace/ot_perturbation/data/sciplex/adata_train_{split}.h5ad\"\n",
    "adata_test_path = f\"/lustre/groups/ml01/workspace/ot_perturbation/data/sciplex/adata_test_{split}.h5ad\"\n",
    "adata_ood_path = f\"/lustre/groups/ml01/workspace/ot_perturbation/data/sciplex/adata_ood_{split}.h5ad\"\n",
    "adata_train = sc.read_h5ad(adata_train_path)\n",
    "adata_test = sc.read_h5ad(adata_test_path)\n",
    "adata_ood = sc.read_h5ad(adata_ood_path)\n",
    "\n",
    "adata_train.uns[\"sample_covariates_one_hot\"] = {\"A549\": 0, \"K562\": 1, \"MCF7\": 2}\n",
    "adata_test.uns[\"sample_covariates_one_hot\"] = {\"A549\": 0, \"K562\": 1, \"MCF7\": 2}\n",
    "adata_ood.uns[\"sample_covariates_one_hot\"] = {\"A549\": 0, \"K562\": 1, \"MCF7\": 2}\n",
    "cf = cfp.model.CellFlow(adata_train, solver=\"otfm\")\n",
    "\n",
    "\n",
    "cf.prepare_data(\n",
    "    sample_rep=\"X_pca\",\n",
    "    control_key=\"control\",\n",
    "    perturbation_covariates={\"drugs\": [\"drug\"], \"dose\": [\"logdose\"]},\n",
    "    perturbation_covariate_reps={\"drugs\": \"ecfp_dict\"},\n",
    "    sample_covariates = [\"cell_line\"],\n",
    "    sample_covariate_reps = {\"cell_line\": \"sample_covariates_one_hot\"},\n",
    "    split_covariates=[\"cell_line\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfca384b-8e8d-4b3a-a698-bb74471c2c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ictstr01/home/icb/dominik.klein/git_repos/cell_flow_perturbation/src/cfp/data/_datamanager.py:334: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  _covariate_data[\"cell_index\"] = _covariate_data.index\n",
      "100%|██████████| 677/677 [00:05<00:00, 131.95it/s]\n",
      "100%|██████████| 708/708 [00:04<00:00, 149.90it/s]\n",
      "100%|██████████| 660/660 [00:04<00:00, 145.70it/s]\n"
     ]
    }
   ],
   "source": [
    "cf.prepare_validation_data(\n",
    "    adata_test,\n",
    "    name=\"test\",\n",
    "    n_conditions_on_log_iteration=None,\n",
    "    n_conditions_on_train_end=None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc1da757-3460-4183-bf40-8f9a5194fd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ictstr01/home/icb/dominik.klein/git_repos/cell_flow_perturbation/src/cfp/data/_datamanager.py:334: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  _covariate_data[\"cell_index\"] = _covariate_data.index\n",
      "100%|██████████| 28/28 [00:00<00:00, 206.56it/s]\n",
      "100%|██████████| 33/33 [00:00<00:00, 203.03it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 201.66it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cf.prepare_validation_data(\n",
    "    adata_ood,\n",
    "    name=\"ood\",\n",
    "    n_conditions_on_log_iteration=None,\n",
    "    n_conditions_on_train_end=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2011b0d2-1e36-4bee-beb0-686c9f0ede32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cfp.data._dataloader import PredictionSampler, TrainSampler, ValidationSampler\n",
    "train_dataloader = TrainSampler(data=cf.train_data, batch_size=batch_size)\n",
    "validation_loaders = {\n",
    "    k: ValidationSampler(v) for k, v in cf.validation_data.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d7efbac-b169-41d5-b5c1-0c2a9969373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = OTTNeuralDualSolver(input_dim=N_PCs, cond_dim=1026, pretrain_iters=pretrain_iters, iterations=iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51b946f4-cbcc-42c6-874d-6235586fb656",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:36<00:00, 273.78it/s]\n",
      "100%|██████████| 200000/200000 [23:43<00:00, 140.47it/s]\n"
     ]
    }
   ],
   "source": [
    "solver(train_dataloader, N_PCs=N_PCs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "daef2951-7722-4d34-8389-1e64625a4142",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = validation_loaders[\"ood\"].sample(mode=\"on_train_end\")\n",
    "\n",
    "batch = {}\n",
    "preds_ood = {}\n",
    "source_dict = out[\"source\"]\n",
    "condition_dict = out[\"condition\"]\n",
    "target_dict = out[\"target\"]\n",
    "for cond in source_dict.keys():\n",
    "    batch[\"source\"]=source_dict[cond][:,:N_PCs]\n",
    "    batch[\"target\"]= target_dict[cond][:,:N_PCs]\n",
    "    condition_arr = jnp.concatenate((condition_dict[cond][\"drugs\"], condition_dict[cond][\"dose\"], condition_dict[cond][\"cell_line\"]), axis=-1)\n",
    "    batch[\"condition\"] = jnp.squeeze(condition_arr[0,...])\n",
    "\n",
    "    preds_ood[cond] = jax.vmap(\n",
    "                jax.grad(lambda x: solver.state_f.apply_fn({\"params\": solver.state_f.params}, x, batch[\"condition\"]), argnums=0)\n",
    "            )(batch[\"source\"])\n",
    "\n",
    "    \n",
    "adapted_preds_ood = {k[0]+\"_\"+str(k[1])+\"_\"+k[2]: v for k,v in preds_ood.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "509b67fa-5731-4d14-a749-440b377af9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = validation_loaders[\"test\"].sample(mode=\"on_train_end\")\n",
    "\n",
    "batch = {}\n",
    "preds_test = {}\n",
    "source_dict = out[\"source\"]\n",
    "condition_dict = out[\"condition\"]\n",
    "target_dict = out[\"target\"]\n",
    "for cond in source_dict.keys():\n",
    "    batch[\"source\"]=source_dict[cond][:,:N_PCs]\n",
    "    batch[\"target\"]= target_dict[cond][:,:N_PCs]\n",
    "    condition_arr = jnp.concatenate((condition_dict[cond][\"drugs\"], condition_dict[cond][\"dose\"], condition_dict[cond][\"cell_line\"]), axis=-1)\n",
    "    batch[\"condition\"] = jnp.squeeze(condition_arr[0,...])\n",
    "\n",
    "    preds_test[cond] = jax.vmap(\n",
    "                jax.grad(lambda x: solver.state_f.apply_fn({\"params\": solver.state_f.params}, x, batch[\"condition\"]), argnums=0)\n",
    "            )(batch[\"source\"])\n",
    "\n",
    "    \n",
    "adapted_preds_test = {k[0]+\"_\"+str(k[1])+\"_\"+k[2]: v for k,v in preds_test.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c10e36ff-0c64-446f-9239-885653196804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icb/dominik.klein/mambaforge/envs/cfp/lib/python3.11/site-packages/anndata/_core/aligned_df.py:67: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import anndata as ad\n",
    "import cfp.preprocessing as cfpp\n",
    "all_data = []\n",
    "conditions = []\n",
    "\n",
    "for condition, array in adapted_preds_ood.items():\n",
    "    all_data.append(array)\n",
    "    conditions.extend([condition] * array.shape[0])\n",
    "    \n",
    "\n",
    "# Stack all data vertically to create a single array\n",
    "all_data_array = np.vstack(all_data)\n",
    "\n",
    "# Create a DataFrame for the .obs attribute\n",
    "obs_data = pd.DataFrame({\n",
    "    'condition': conditions\n",
    "})\n",
    "\n",
    "# Create the Anndata object\n",
    "adata_ood_result = ad.AnnData(X=np.empty((len(all_data_array),adata_train.n_vars)), obs=obs_data)\n",
    "adata_ood_result.obsm[\"X_pca_pred\"] = np.concatenate((all_data_array, np.zeros((len(all_data_array), 300-N_PCs))), axis=1)\n",
    "cfpp.reconstruct_pca(query_adata=adata_ood_result, use_rep=\"X_pca_pred\", ref_adata=adata_train, layers_key_added=\"X_recon_pred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da4a7442-a822-4563-a6ef-5ce02711d5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icb/dominik.klein/mambaforge/envs/cfp/lib/python3.11/site-packages/anndata/_core/aligned_df.py:67: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import anndata as ad\n",
    "import cfp.preprocessing as cfpp\n",
    "all_data = []\n",
    "conditions = []\n",
    "\n",
    "for condition, array in adapted_preds_test.items():\n",
    "    all_data.append(array)\n",
    "    conditions.extend([condition] * array.shape[0])\n",
    "    \n",
    "\n",
    "# Stack all data vertically to create a single array\n",
    "all_data_array = np.vstack(all_data)\n",
    "\n",
    "# Create a DataFrame for the .obs attribute\n",
    "obs_data = pd.DataFrame({\n",
    "    'condition': conditions\n",
    "})\n",
    "\n",
    "# Create the Anndata object\n",
    "adata_test_result = ad.AnnData(X=np.empty((len(all_data_array),adata_train.n_vars)), obs=obs_data)\n",
    "adata_test_result.obsm[\"X_pca_pred\"] = np.concatenate((all_data_array, np.zeros((len(all_data_array), 300-N_PCs))), axis=1)\n",
    "cfpp.reconstruct_pca(query_adata=adata_test_result, use_rep=\"X_pca_pred\", ref_adata=adata_train, layers_key_added=\"X_recon_pred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed7c1688-ae45-4ed1-8a38-016e02573c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_first_last_underscore(s):\n",
    "    last_underscore = s.rfind('_')\n",
    "    second_last_underscore = s[:last_underscore].rfind('_')\n",
    "    \n",
    "    if last_underscore == -1 or second_last_underscore == -1:\n",
    "        return s, None, None  # If there are less than two underscores\n",
    "    \n",
    "    first_part = s[:second_last_underscore]\n",
    "    middle_part = s[second_last_underscore + 1:last_underscore]\n",
    "    last_part = s[last_underscore + 1:]\n",
    "    \n",
    "    return first_part, middle_part, last_part\n",
    "    \n",
    "def get_condition(x):\n",
    "    drug, dose, cell_line = split_by_first_last_underscore(x[\"condition\"])\n",
    "    return cell_line+\"_\"+drug+\"_\"+str(float(10**int(float(dose))))\n",
    "    \n",
    "    \n",
    "adata_ood_result.obs[\"condition\"] = adata_ood_result.obs.apply(get_condition, axis=1).astype(\"category\")\n",
    "adata_test_result.obs[\"condition\"] = adata_test_result.obs.apply(get_condition, axis=1).astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11105a33-d759-452e-9c79-d04e0e3de489",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_ood_result.write(f\"/lustre/groups/ml01/workspace/ot_perturbation/models/condot/sciplex/adata_ood_with_predictions_{split}.h5ad\")\n",
    "adata_test_result.write(f\"/lustre/groups/ml01/workspace/ot_perturbation/models/condot/sciplex/adata_test_with_predictions_{split}.h5ad\")\n",
    "/lustre/groups/ml01/workspace/ot_perturbation/models/condot/sciplex/adata_test_with_predictions\n",
    "/lustre/groups/ml01/workspace/ot_perturbation/models/condot/sciplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8cb79b82-cb38-431d-9e04-2172ed8c0055",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_pred_test = sc.read(f\"/lustre/groups/ml01/workspace/ot_perturbation/models/condot/sciplex/adata_test_with_predictions_{split}.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eef0aa4-a599-4b68-ba9b-7691ac978f3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cfp",
   "language": "python",
   "name": "cfp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
