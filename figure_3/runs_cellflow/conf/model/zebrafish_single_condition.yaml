condition_embedding_dim: 256
time_encoder_dims: [1024, 1024, 1024]
time_encoder_dropout: 0.0
hidden_dims: [2048, 2048, 2048]
hidden_dropout: 0.0
decoder_dims: [4096, 4096, 4096]
decoder_dropout: 0.0
pooling: "attention_token"
layers_before_pool: 
  gene_knockout:
    layer_type: mlp
    dims: [512, 512]
    dropout_rate: 0.0
  logtime:
    layer_type: mlp
    dims: [512, 512]
    dropout_rate: 0.0
layers_after_pool:
  layer_type: mlp
  dims: [1024, 1024]
  dropout_rate: 0.0
cond_output_dropout: 0.9
time_freqs: 1024
flow_noise: 0.5
learning_rate: 0.00005
multi_steps: 20
epsilon: 0.5
tau_a: 1.0
tau_b: 1.0
flow_type: "constant_noise"
linear_projection_before_concatenation: False
layer_norm_before_concatenation: False